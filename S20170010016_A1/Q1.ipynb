{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "bH1XtERSkkH7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Question 1 - Assignment 1"
      ]
    },
    {
      "metadata": {
        "id": "FB2Jws7SkonU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The first task is to design a Multi-Layer Perceptron for XOR gate operation.\n",
        "\n",
        "The Multi-Layer Perceptron (MLP) for XOR gate operation can be visually analyzed with 2 input neurons, 2 hidden neurons, and 1 output neuron. Along with this, the weights are learned through the Stochastic Gradient Descent. For activations, I have used the Tanh function whose range lies from (-1, 1). The model was trained for 10000 epochs with a learning rate of 0.1.\n",
        "\n",
        "The output/predictions largely depend upon the initialization of the weights and biases which have led to varied results."
      ]
    },
    {
      "metadata": {
        "id": "KFelUBocj4LQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ]
    },
    {
      "metadata": {
        "id": "VK7iMV47kIGw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Of5hAACHj8id",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tanh Activation Function and its Derivative"
      ]
    },
    {
      "metadata": {
        "id": "SSNPDO8Aj3i6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tanh(x):\n",
        "    return (1.0 - np.exp(-2*x))/(1.0 + np.exp(-2*x))\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return (1 + tanh(x))*(1 - tanh(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pMYW7q67kEkz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Variable Intialization"
      ]
    },
    {
      "metadata": {
        "id": "3x1-nxZhj1l5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "epoch = 10000\n",
        "lr = 0.1\n",
        "inputlayer_neurons = X.shape[1]\n",
        "hiddenlayer_neurons = 2 \n",
        "outputlayer_neurons = 1\n",
        "\n",
        "weight_hidden = np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))\n",
        "bias_hidden = np.random.uniform(size=(1,hiddenlayer_neurons))\n",
        "weight_out = np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
        "bias_out = np.random.uniform(size=(1,output_neurons))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Riv71FbqkOYd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train the Network"
      ]
    },
    {
      "metadata": {
        "id": "KdDUP1cxkT2a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(epoch):\n",
        "\n",
        "  #Forward-Pass\n",
        "  \n",
        "  hidden_layer_input = np.dot(X,weight_hidden)\n",
        "  hidden_layer_input = hidden_layer_input + bias_hidden\n",
        "  hiddenlayer_activations = sigmoid(hidden_layer_input)\n",
        "  output_layer_input = np.dot(hiddenlayer_activations,weight_out)\n",
        "  output_layer_input= output_layer_input + bias_out\n",
        "  output = tanh(output_layer_input)\n",
        "\n",
        "  #Back-Propagation\n",
        "  \n",
        "  error = y - output\n",
        "  slope_output_layer = tanh_derivative(output)\n",
        "  slope_hidden_layer = tanh_derivative(hiddenlayer_activations)\n",
        "  d_output = error * slope_output_layer\n",
        "  error_at_hidden_layer = d_output.dot(weight_out.T)\n",
        "  d_hiddenlayer = error_at_hidden_layer * slope_hidden_layer\n",
        "  \n",
        "  #Stochastic-Gradient-Descent\n",
        "  \n",
        "  weight_out += hiddenlayer_activations.T.dot(d_output) * lr\n",
        "  bias_out += np.sum(d_output, axis=0,keepdims=True) * lr\n",
        "  weight_hidden += X.T.dot(d_hiddenlayer) * lr\n",
        "  bias_hidden += np.sum(d_hiddenlayer, axis=0, keepdims=True) * lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v3ySuA6DkdaW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Display Output"
      ]
    },
    {
      "metadata": {
        "id": "kiYlhtNukah1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(output)\n",
        "print([i[0] > 0 for i in output])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}