{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bH1XtERSkkH7"
   },
   "source": [
    "# Question 1 - Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FB2Jws7SkonU"
   },
   "source": [
    "The first task is to design a Multi-Layer Perceptron for XOR gate operation.\n",
    "\n",
    "The Multi-Layer Perceptron (MLP) for XOR gate operation can be visually analyzed with 2 input neurons, 2 hidden neurons, and 1 output neuron. Along with this, the weights are learned through the Stochastic Gradient Descent. For activations, I have used the Tanh function whose range lies from (-1, 1). The model was trained for 10000 epochs with a learning rate of 0.1.\n",
    "\n",
    "The output/predictions largely depend upon the initialization of the weights and biases which have led to varied results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KFelUBocj4LQ"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VK7iMV47kIGw"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Of5hAACHj8id"
   },
   "source": [
    "### Tanh Activation Function and its Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SSNPDO8Aj3i6"
   },
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return (1.0 - np.exp(-2*x))/(1.0 + np.exp(-2*x))\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 + tanh(x))*(1 - tanh(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pMYW7q67kEkz"
   },
   "source": [
    "### Variable Intialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3x1-nxZhj1l5"
   },
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "epoch = 10000\n",
    "lr = 0.1\n",
    "inputlayer_neurons = X.shape[1]\n",
    "hiddenlayer_neurons = 2 \n",
    "outputlayer_neurons = 1\n",
    "\n",
    "weight_hidden = np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))\n",
    "bias_hidden = np.random.uniform(size=(1,hiddenlayer_neurons))\n",
    "weight_out = np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
    "bias_out = np.random.uniform(size=(1,output_neurons))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Riv71FbqkOYd"
   },
   "source": [
    "### Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KdDUP1cxkT2a"
   },
   "outputs": [],
   "source": [
    "for i in range(epoch):\n",
    "\n",
    "  #Forward-Pass\n",
    "  \n",
    "  hidden_layer_input = np.dot(X,weight_hidden)\n",
    "  hidden_layer_input = hidden_layer_input + bias_hidden\n",
    "  hiddenlayer_activations = sigmoid(hidden_layer_input)\n",
    "  output_layer_input = np.dot(hiddenlayer_activations,weight_out)\n",
    "  output_layer_input= output_layer_input + bias_out\n",
    "  output = tanh(output_layer_input)\n",
    "\n",
    "  #Back-Propagation\n",
    "  \n",
    "  error = y - output\n",
    "  slope_output_layer = tanh_derivative(output)\n",
    "  slope_hidden_layer = tanh_derivative(hiddenlayer_activations)\n",
    "  d_output = error * slope_output_layer\n",
    "  error_at_hidden_layer = d_output.dot(weight_out.T)\n",
    "  d_hiddenlayer = error_at_hidden_layer * slope_hidden_layer\n",
    "  \n",
    "  #Stochastic-Gradient-Descent\n",
    "  \n",
    "  weight_out += hiddenlayer_activations.T.dot(d_output) * lr\n",
    "  bias_out += np.sum(d_output, axis=0,keepdims=True) * lr\n",
    "  weight_hidden += X.T.dot(d_hiddenlayer) * lr\n",
    "  bias_hidden += np.sum(d_hiddenlayer, axis=0, keepdims=True) * lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v3ySuA6DkdaW"
   },
   "source": [
    "### Display Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kiYlhtNukah1"
   },
   "outputs": [],
   "source": [
    "print(output)\n",
    "print([i[0] > 0 for i in output])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MLP.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
