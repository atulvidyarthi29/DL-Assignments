{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B1_j-6kDuCI6"
   },
   "source": [
    "# Question 3 - Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TOdri3kluCI8"
   },
   "source": [
    "The third task is to provide the sentiment analysis of the text obtained from the Rotten Tomatoes dataset. The sentiments can be classified into 5 categories.\n",
    "\n",
    "The task is achieved using a 3-layer LSTM with a fully connected layer to give the 5 outputs. Cross-Entropy Loss function is used along with the Stochastic Gradient Descent optimizer.\n",
    "\n",
    "The code has been written using the PyTorch Deep Learning Framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eNHdOgaouCI9"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xHC_xr74Ogae"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PMDSpo8MuCJB"
   },
   "source": [
    "### LSTM Class Definition for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZFrs3_DjuCJC"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocabulary_size, output_size, embedding_dimension, hidden_dimension, number_layers, drop_prob=0.5):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.number_layers = number_layers\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocabulary_size, embedding_dimension)\n",
    "        self.lstm = nn.LSTM(embedding_dimension, hidden_dimension, number_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dimension, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden_state):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        embeddings = self.embedding(x)\n",
    "        lstm_output, hidden_state = self.lstm(embeddings, hidden_state)\n",
    "        \n",
    "        lstm_output = lstm_output[:,-1,:]\n",
    "        output = self.dropout(lstm_output)\n",
    "        output = self.fc(output)\n",
    "        sigmoid_output = self.sig(output)\n",
    "        \n",
    "        sigmoid_output = sigmoid_output.view(batch_size, -1)\n",
    "        \n",
    "        return sigmoid_output, hidden_state\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if gpu:\n",
    "            hidden_state = (weight.new(self.number_layers, batch_size, self.hidden_dimension).zero_().cuda(),\n",
    "                  weight.new(self.number_layers, batch_size, self.hidden_dimension).zero_().cuda())\n",
    "        else:\n",
    "            hidden_state = (weight.new(self.number_layers, batch_size, self.hidden_dimension).zero_(),\n",
    "                      weight.new(self.number_layers, batch_size, self.hidden_dimension).zero_())\n",
    "        \n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gC-rmW5CuCJF"
   },
   "source": [
    "### Helper Functions for Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xpo64aO7QaYA"
   },
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "   \n",
    "    text = str(text).lower()\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "    text = BAD_SYMBOLS_RE.sub('', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return text\n",
    "\n",
    "def padding_features(string_int, sequence_length=100):\n",
    "    \n",
    "    features = np.zeros((len(string_int), sequence_length), dtype = int)\n",
    "    \n",
    "    for i, string in enumerate(string_int):\n",
    "        \n",
    "        string_len = len(string)\n",
    "        \n",
    "        if string_len <= sequence_length:\n",
    "            \n",
    "            zeroes = list(np.zeros(sequence_length-string_len))\n",
    "            new = zeroes+string\n",
    "        \n",
    "        elif string_len > sequence_length:\n",
    "            \n",
    "            new = string[0:sequence_length]\n",
    "        \n",
    "        features[i,:] = np.array(new)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u9ZOf6uduCJL"
   },
   "source": [
    "### Read Data & Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hxzyYwtUPL_T"
   },
   "outputs": [],
   "source": [
    "data = pd.read_table('mrdata.tsv')\n",
    "data['Phrase'] = data['Phrase'].apply(clean_text)\n",
    "del data['PhraseId']\n",
    "del data['SentenceId']\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v3e94S4ruCJR"
   },
   "source": [
    "### Encoded Text to Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TtP6QcqoTo1w"
   },
   "outputs": [],
   "source": [
    "all_text = ' '.join(data['Phrase'])\n",
    "words = all_text.split()\n",
    "count_words = Counter(words)\n",
    "total_words = len(words)\n",
    "sorted_words = count_words.most_common(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tzYHCX6aUPO_"
   },
   "outputs": [],
   "source": [
    "vocabulary_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
    "string_int = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TLN5RE3YuCJZ"
   },
   "outputs": [],
   "source": [
    "for string in data['Phrase']:\n",
    "    \n",
    "    r = [vocabulary_to_int[w] for w in string.split()]\n",
    "    string_int.append(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7d9D40XkuCJc"
   },
   "source": [
    "### Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vbo8_vPTVCUE"
   },
   "outputs": [],
   "source": [
    "encoded_labels = np.array([int(i) for i in data['Sentiment']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GBC53iGbuCJh"
   },
   "source": [
    "### Truncate & Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pN3TgOniWFIu"
   },
   "outputs": [],
   "source": [
    "string_length = [len(x) for x in string_int]\n",
    "string_int = [string_int[i] for i, l in enumerate(string_length) if l>0]\n",
    "encoded_labels = [encoded_labels[i] for i, l in enumerate(string_length) if l>0]\n",
    "features = padding_features(string_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Z6nWmlVuCJo"
   },
   "source": [
    "### Split Data into Train & Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CoUJC9l0aJ4O"
   },
   "outputs": [],
   "source": [
    "split_frac = 0.8\n",
    "len_feat = len(features)\n",
    "train_x = np.array(features[0:int(split_frac*len_feat)])\n",
    "train_y = np.array(encoded_labels[0:int(split_frac*len_feat)])\n",
    "test_x = np.array(features[int(split_frac*len_feat):])\n",
    "test_y = np.array(encoded_labels[int(split_frac*len_feat):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6pTtcL9racST"
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6F5drVe_uCJv"
   },
   "source": [
    "### Variable Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kd3vJ2chF-vy"
   },
   "outputs": [],
   "source": [
    "vocabulary_size = len(vocabulary_to_int)+1\n",
    "output_size = 5\n",
    "embedding_dimension = 400\n",
    "hidden_dimension = 256\n",
    "number_layers = 3\n",
    "gpu = False\n",
    "lr=0.001\n",
    "net = LSTM(vocabulary_size, output_size, embedding_dimension, hidden_dimension, number_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ro9b2tHpuCJy"
   },
   "source": [
    "### Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gzFv_EIlGZRa"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "counter = 0\n",
    "clip = 5\n",
    "\n",
    "if gpu:\n",
    "  net.cuda()\n",
    "\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1pxuyQ9iuCJ2"
   },
   "source": [
    "### Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_mQiKd0vuCJ3"
   },
   "outputs": [],
   "source": [
    "for e in range(epochs):\n",
    "    \n",
    "    hidden_state = net.init_hidden(batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        \n",
    "        counter += 1\n",
    "\n",
    "        if gpu:\n",
    "          inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        hidden_state = tuple([each.data for each in hidden_state])\n",
    "\n",
    "        net.zero_grad()\n",
    "\n",
    "        inputs = inputs.type(torch.LongTensor)\n",
    "        \n",
    "        if((inputs.shape[0],inputs.shape[1]) != (batch_size,100)):\n",
    "          \n",
    "          print('Input Shape Issue:', inputs.shape)\n",
    "          continue\n",
    "        \n",
    "        output, hidden_state = net(inputs, hidden_state)\n",
    "        \n",
    "        loss = criterion(output.squeeze(), labels)\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\"Epoch: {}/{}...\".format(e+1, epochs), \"Step: {}...\".format(counter), \"Loss: {:.6f}...\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hUkrLrPEuCJ6"
   },
   "source": [
    "### Compute Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jsGZ1O51HVEz"
   },
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "num_correct = 0\n",
    "\n",
    "hidden_state = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    hidden_state = tuple([each.data for each in hidden_state])\n",
    "\n",
    "    if gpu:\n",
    "      inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    inputs = inputs.type(torch.LongTensor)\n",
    "    \n",
    "    if((inputs.shape[0],inputs.shape[1]) != (batch_size,100)):\n",
    "          \n",
    "          print('Input Shape Issue:', inputs.shape)\n",
    "          continue\n",
    "    \n",
    "    output, hidden_state = net(inputs, hidden_state)\n",
    "    \n",
    "    test_loss = criterion(output.squeeze(), labels)\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    out = F.softmax(output)\n",
    "    out = out.max(dim=1)[1]\n",
    "    \n",
    "    correct_tensor = out.eq(labels)\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "print(\"Test Loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "test_accuracy = num_correct/len(test_loader.dataset)\n",
    "print(\"Test Accuracy: {:.3f}\".format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "eNHdOgaouCI9",
    "PMDSpo8MuCJB",
    "gC-rmW5CuCJF",
    "u9ZOf6uduCJL",
    "v3e94S4ruCJR",
    "7d9D40XkuCJc",
    "GBC53iGbuCJh",
    "1Z6nWmlVuCJo",
    "6F5drVe_uCJv",
    "Ro9b2tHpuCJy",
    "hUkrLrPEuCJ6"
   ],
   "name": "Assignment-1-Q3.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
